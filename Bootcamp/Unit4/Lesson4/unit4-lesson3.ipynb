{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit 4 lesson 4.3 - unsupervised problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to C:\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinkful's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ Emma by Jane Austen 1816 ]', 'VOLUME I', 'CHAPTER I', 'Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her .']\n"
     ]
    }
   ],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "#processing\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    \n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    \n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "print(emma_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1948\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "Tf_idf vector: {'minutes': 0.7127450310382584, 'present': 0.701423210857947}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, # convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True, # we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', # Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True # Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 45.19623698007339\n",
      "\n",
      "Component 0:\n",
      "\" Oh !\"    0.999293\n",
      "\" Oh !     0.999293\n",
      "\" Oh !     0.999293\n",
      "\" Oh !     0.999293\n",
      "\" Oh !     0.999293\n",
      "\" Oh !\"    0.999293\n",
      "\" Oh !     0.999293\n",
      "\" Oh !     0.999293\n",
      "\" Oh !     0.999293\n",
      "\" Oh !     0.999293\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .                                                                                                                0.633777\n",
      "\" You get upon delicate subjects , Emma ,\" said Mrs . Weston smiling ; \" remember that I am here . Mr .                                                                     0.582415\n",
      "\" I do not know what your opinion may be , Mrs . Weston ,\" said Mr . Knightley , \" of this great intimacy between Emma and Harriet Smith , but I think it a bad thing .\"    0.564514\n",
      "\" You are right , Mrs . Weston ,\" said Mr . Knightley warmly , \" Miss Fairfax is as capable as any of us of forming a just opinion of Mrs . Elton .                         0.562711\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                                 0.528124\n",
      "Mr . Knightley might quarrel with her , but Emma could not quarrel with herself .                                                                                           0.525484\n",
      "\" Now ,\" said Emma , when they were fairly beyond the sweep gates , \" now Mr . Weston , do let me know what has happened .\"                                                 0.515764\n",
      "\" In one respect , perhaps , Mr . Elton ' s manners are superior to Mr . Knightley ' s or Mr . Weston ' s .                                                                 0.504608\n",
      "Emma could not have desired a more spirited rejection of Mr . Martin ' s prose .                                                                                            0.501054\n",
      "Emma found that it was not Mr . Weston ' s fault that the number of privy councillors was not yet larger .                                                                  0.499946\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "CHAPTER X      0.998837\n",
      "CHAPTER V      0.998837\n",
      "CHAPTER V      0.998837\n",
      "CHAPTER I      0.998837\n",
      "CHAPTER X      0.998837\n",
      "CHAPTER I      0.998837\n",
      "CHAPTER V      0.998837\n",
      "CHAPTER X      0.998837\n",
      "CHAPTER I      0.998837\n",
      "CHAPTER XII    0.997959\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "\" Ah !    0.992896\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .    0.650309\n",
      "\" Are you well , my Emma ?\"                                                    0.598067\n",
      "Emma demurred .                                                                0.598067\n",
      "Emma was silenced .                                                            0.586188\n",
      "At first it was downright dulness to Emma .                                    0.585203\n",
      "\" Emma , my dear Emma \"                                                        0.576537\n",
      "Emma could not resist .                                                        0.567105\n",
      "\" It is not now worth a regret ,\" said Emma .                                  0.546078\n",
      "\" For shame , Emma !                                                           0.544123\n",
      "\" No great variety of faces for you ,\" said Emma .                             0.491816\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 0:\n",
    "Apply LSA to the test set. Does it identify similar sentences for components 0 through 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "\n",
      "Tf_idf vector: {'resolving': 0.3424390305045805, 'unexceptionable': 0.3196409792117937, 'assure': 0.267729777836069, 'friends': 0.2515333740641343, 'quite': 0.1916471918420347, 'replied': 0.20315405638674577, 'watch': 0.62300800500314, 'elton': 0.16874782424809767, 'pass': 0.28445227585106625, 'really': 0.21963852817625326, 'mrs': 0.14780961753993585}\n"
     ]
    }
   ],
   "source": [
    "#Reshapes the test vectorizer output into something people can read\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_test_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_test_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_test_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('\\nTf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 49.36813400029567\n",
      "\n",
      "Component 0:\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh no !    0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh no !    0.99992\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "\" Well , Mrs . Weston ,\" said Emma triumphantly when he left them , \" what do you say now to Mr . Knightley ' s marrying Jane Fairfax ?\"                                                                                                                                                                                                                                                                                                             0.616151\n",
      "After tea , Mr . and Mrs . Weston , and Mr . Elton sat down with Mr . Woodhouse to cards .                                                                                                                                                                                                                                                                                                                                                           0.602247\n",
      "Frank turned instantly to Emma , to claim her former promise ; and boasted himself an engaged man , which his father looked his most perfect approbation of  and it then appeared that Mrs . Weston was wanting _him_ to dance with Mrs . Elton himself , and that their business was to help to persuade him into it , which was done pretty soon . Mr . Weston and Mrs . Elton led the way , Mr . Frank Churchill and Miss Woodhouse followed .    0.566174\n",
      "\" Mr .                                                                                                                                                                                                                                                                                                                                                                                                                                               0.518388\n",
      "While she was gone , Mr . Knightley called , and sat some time with Mr . Woodhouse and Emma , till Mr . Woodhouse , who had previously made up his mind to walk out , was persuaded by his daughter not to defer it , and was induced by the entreaties of both , though against the scruples of his own civility , to leave Mr . Knightley for that purpose .                                                                                       0.513761\n",
      "Mr . Weston was musing .                                                                                                                                                                                                                                                                                                                                                                                                                             0.505118\n",
      "\" Mrs . Weston ' s manners ,\" said Emma , \" were always particularly good .                                                                                                                                                                                                                                                                                                                                                                          0.494415\n",
      "\" I think , indeed ,\" said John Knightley pleasantly , \" that Mr . Weston has some little claim .                                                                                                                                                                                                                                                                                                                                                    0.488987\n",
      "\" It is Frank and Miss Fairfax ,\" said Mrs . Weston .                                                                                                                                                                                                                                                                                                                                                                                                0.485398\n",
      "\" Why , to be sure ,\" said Mr . Woodhouse \" yes , certainly  I cannot deny that Mrs . Weston , poor Mrs . Weston , does come and see us pretty often  but then  she is always obliged to go away again .\"                                                                                                                                                                                                                                            0.485290\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "\" Ah !     0.99697\n",
      "\" Ah !     0.99697\n",
      "\" Ah !     0.99697\n",
      "\" Ah !     0.99697\n",
      "\" Ah !     0.99697\n",
      "\" Ah !\"    0.99697\n",
      "\" Ah !\"    0.99697\n",
      "\" Ah !     0.99697\n",
      "\" Ah !     0.99697\n",
      "\" Ah !     0.99697\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "\" Mr .                                                                                                                                                                                                                                                                                                                                                            0.632547\n",
      "After tea , Mr . and Mrs . Weston , and Mr . Elton sat down with Mr . Woodhouse to cards .                                                                                                                                                                                                                                                                        0.566261\n",
      "Mr . Knightley was thoughtful again .                                                                                                                                                                                                                                                                                                                             0.524020\n",
      "\" You are not vain , Mr . Knightley .                                                                                                                                                                                                                                                                                                                             0.508261\n",
      "Mr . Weston was musing .                                                                                                                                                                                                                                                                                                                                          0.499730\n",
      "Mr . Weston ' s own happiness was indisputable .                                                                                                                                                                                                                                                                                                                  0.452180\n",
      "Harriet , Mr . Elton , and Mr . Knightley , their own especial set , were the only persons invited to meet them ; the hours were to be early , as well as the numbers few ; Mr . Woodhouse ' s habits and inclination being consulted in every thing .                                                                                                            0.418603\n",
      "\" And I , Mr . Knightley , am equally stout in my confidence of its not doing them any harm .                                                                                                                                                                                                                                                                     0.405015\n",
      "She meant to be very happy , in spite of the scene being laid at Mr . Cole ' s ; and without being able to forget that among the failings of Mr . Elton , even in the days of his favour , none had disturbed her more than his propensity to dine with Mr . Cole .                                                                                               0.401694\n",
      "While she was gone , Mr . Knightley called , and sat some time with Mr . Woodhouse and Emma , till Mr . Woodhouse , who had previously made up his mind to walk out , was persuaded by his daughter not to defer it , and was induced by the entreaties of both , though against the scruples of his own civility , to leave Mr . Knightley for that purpose .    0.391771\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "\" Yes , sometimes he can .\"                                                     0.653158\n",
      "\" Yes , do .\"                                                                   0.653158\n",
      "\" Yes .                                                                         0.653158\n",
      "\" Yes I should , I am sure I should .                                           0.639064\n",
      "\" Yes , so I imagined .                                                         0.606356\n",
      "\" Yes  a good deal _nearer_ .\"                                                  0.567058\n",
      "\" Yes , our good Mrs . Elton .                                                  0.553534\n",
      "\" Yes ; but we must not rest our claims on that distinction .                   0.547143\n",
      "\" Yes , she would be , but that she thinks there will be another put - off .    0.535666\n",
      "\" Yes , very soon .                                                             0.533857\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Run SVD on the training data, then project the training data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "\n",
      " 0 Mr . Woodhouse had so completely made up his mind to the visit , that in spite of the increasing coldness , he seemed to have no idea of shrinking from it , and set forward at last most punctually with his eldest daughter in his own carriage , with less apparent consciousness of the weather than either of the others ; too full of the wonder of his own going , and the pleasure it was to afford at Randalls to see that it was cold , and too well wrapt up to feel it .\n",
      "\n",
      " 1 \" Oh !\n",
      "\n",
      " 2 \" Oh no , no !\n",
      "\n",
      " 3 Such was Jane Fairfax ' s history .\n",
      "\n",
      " 4 \" That has been a good deal the case , my dear ; but not to the degree you mention .\n",
      "\n",
      " 5 \" And I am quite serious too , I assure you ,\" replied Mrs . Elton gaily , \" in resolving to be always on the watch , and employing my friends to watch also , that nothing really unexceptionable may pass us .\"\n",
      "\n",
      " 6 \" And here is Mrs . Weston and Mr . Frank Churchill too ! Quite delightful ; so many friends !\"\n",
      "\n",
      " 7 \" You may well class the delight , the honour , and the comfort of such a situation together ,\" said Jane , \" they are pretty sure to be equal ; however , I am very serious in not wishing any thing to be attempted at present for me .\n",
      "\n",
      " 8 Harriet , Mr . Elton , and Mr . Knightley , their own especial set , were the only persons invited to meet them ; the hours were to be early , as well as the numbers few ; Mr . Woodhouse ' s habits and inclination being consulted in every thing .\n",
      "\n",
      " 9 \" Oh !\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_test_lsa) * np.asmatrix(X_test_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_test).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print('\\n',i,sim_matrix.index[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least twice\n",
    "                             stop_words='english',\n",
    "                             strip_accents ='unicode', # Remove accents and perform other character normalization\n",
    "                             lowercase=True, # convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True, # we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm='l1', # Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             #smooth_idf=True # Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1358\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "Tf_idf vector: {'minutes': 0.5040029961941199, 'present': 0.49599700380588013}\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 65.32642744375367\n",
      "\n",
      "Component 0:\n",
      "\" Oh !     0.999928\n",
      "\" Oh !     0.999928\n",
      "\" Oh !\"    0.999928\n",
      "\" Oh !     0.999928\n",
      "\" Oh !     0.999928\n",
      "\" Oh !     0.999928\n",
      "\" Oh !\"    0.999928\n",
      "\" Oh !     0.999928\n",
      "\" Oh !\"    0.999928\n",
      "\" Oh !     0.999928\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "\" Ah !     0.999985\n",
      "\" Ah !     0.999985\n",
      "\" Ah !\"    0.999985\n",
      "\" Ah !     0.999985\n",
      "\" Ah !     0.999985\n",
      "\" Ah !     0.999985\n",
      "\" Ah !     0.999985\n",
      "\" Ah !     0.999985\n",
      "\" Ah !     0.999985\n",
      "\" Ah !     0.999985\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "CHAPTER I       0.997356\n",
      "CHAPTER X       0.997356\n",
      "CHAPTER I       0.997356\n",
      "CHAPTER V       0.997356\n",
      "CHAPTER I       0.997356\n",
      "CHAPTER V       0.997356\n",
      "CHAPTER V       0.997356\n",
      "CHAPTER X       0.997356\n",
      "CHAPTER X       0.997356\n",
      "CHAPTER XVII    0.994346\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                               0.901679\n",
      "\" It is not now worth a regret ,\" said Emma .                                                                                                                             0.898062\n",
      "\" My dearest Emma ,\" said he , \" for dearest you will always be , whatever the event of this hour ' s conversation , my dearest , most beloved Emma  tell me at once .    0.858887\n",
      "\" It is not fair ,\" said Emma , in a whisper ; \" mine was a random guess .                                                                                                0.856575\n",
      "\" I am ready ,\" said Emma , \" whenever I am wanted .\"                                                                                                                     0.855555\n",
      "\" From something that he said , my dear Emma , I rather imagine \"                                                                                                         0.787180\n",
      "\" But you will come again ,\" said Emma .                                                                                                                                  0.742684\n",
      "\" Are you well , my Emma ?\"                                                                                                                                               0.729365\n",
      "Emma was silenced .                                                                                                                                                       0.729365\n",
      "Emma demurred .                                                                                                                                                           0.729365\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "\" We were too magnificent ,\" said he .                                                          0.729840\n",
      "\" He trifles here ,\" said he , \" as to the temptation .                                         0.729840\n",
      "\" I see how it is ,\" said she .                                                                 0.729840\n",
      "\" It is a most repulsive quality , indeed ,\" said he .                                          0.729840\n",
      "\" You were speaking ,\" said he , gravely .                                                      0.709789\n",
      "It was kindly said , and very far from giving offence .                                         0.707013\n",
      "\" That fellow ,\" said he , indignantly , \" thinks of nothing but shewing off his own voice .    0.698638\n",
      "\" You may depend upon me ,\" said she .                                                          0.683425\n",
      "\" What an excellent device ,\" said he , \" the use of a sheepskin for carriages .                0.666181\n",
      "He shook his head ; but there was a smile of indulgence with it , and he only said ,            0.654514\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
